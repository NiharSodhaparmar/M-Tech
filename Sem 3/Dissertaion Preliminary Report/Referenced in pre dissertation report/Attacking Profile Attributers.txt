\section{Attacking Profile Attribute Predictors}

Several protective methodologies have been suggested to anonymize social media data generated by users. In this section we go over how we can anonymize user data. So that the profile attribute predictors can not infer the users' attribute.

The authors of \cite{alufaisan2017hacking} discuss the issue of poisoning and evasion attacks on inference models for profile attributes that are dependent on users' liked pages on Facebook. In evasion attacks, the user modifies their profile data by adding or deleting features (Liking/Unliking pages) that influence a certain attribute, that could compromise users' privacy. In Poisoning Attacks, manipulation of the training data is necessary to reduce the overall performance of the target classifier. In Poisoning Attacks, manipulation of the training data is necessary to reduce the overall performance of the target classifier. To reduce the total target classifier performance, 1\% of the training data must be manipulated. But generally training data of attribute predictor models is not accessible, particularly for black-box models.

Various papers examine adversarial attacks focused on user-generated data, relatively few address attacks on profile attribute classifiers that uses the reactions generated by other users. In \cite{le2020malcom}, authors suggested a method to generate and appending malicious comments to a news story, which would lead a neural fake news detector to incorrectly identify it as FAKE or REAL news. They propose MALCOM, a Malicious Comment Generation Framework, to attack fake news detection machine learning models. Malcom uses a conditional text generator to generates malicious comments. Then, they appends this generated comments to the targeted article.

Generating text in an adversarial environment, where the goal is to target machine learning classifiers, is more difficult because text is discrete. A lot of work has gone into generating adversarial samples to target text based machine learning algorithms \cite{jin2020bert} \cite{li2018textbugger} \cite{mathai2020adversarial}. The majority of them concentrate on making small adjustments (such additions, deletions, or replacements) to the character \cite{li2018textbugger} \cite{mathai2020adversarial} or to the word \cite{jin2020bert} \cite{li2018textbugger}. Even while these techniques work well, they are primarily intended to target static features, such changing a review's content to trick a sentiment analysis classifier. They are not designed to handle sequential dynamic input, such as comments, where content might be added gradually.

To overcome these autohrs in \cite{belhadj2021fox} provide a framework for creating adversarial responses that can successfully target both dynamic sequential inputs (like comments) and static inputs (like alt-text) where the original input shouldn't be changed. With the help of an explainability tool (like LIME) and an initial dataset of Facebook picture reactions, they present a strong framework that can extract powerful adversarial features from reactions. Using these features, adversarial responses will be created for fooling black-box classifiers and prevent them from violating the privacy of users of social networks. 

Another work in \cite{yue2021differential} proposed to hide the sensitive information of text from Machine Learning models using Differential Privacy. They propose a sanitization-aware pretraining procedure. First, they use mechanisms to sanitize public texts, and then they train the Language Model(LM) using the sanitized text. They introduced two token-wise sanitization methods: $SANTEXT$ and $SANTEXT^+$

Drawing inspiration from \cite{belhadj2021fox} and \cite{yue2021differential}, we subsequently introduce a novel approach to posting comments under the framework of Differential Privacy. We describe the FOX framework and Sanitization methods using Differential Privacy in the following sections.

@inproceedings{jin2020bert,
  title={Is bert really robust? a strong baseline for natural language attack on text classification and entailment},
  author={Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={8018--8025},
  year={2020}
}